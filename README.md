# CoolShell-ebook
将陈皓先生 CoolShell 网站上的文章保存为电子书格式方便后人阅读。

爬取博客上所有文章的思路：

1. 爬取所有文章的链接
   - 观察博客的存储路径可以看到 CoolShell 上博客的博客共有 74 页，其中每一页有十个链接
   - 通过 Requests 爬取页面数据后放到 BeautifulSoup 解析即可获取到所有页面的文章链接
2. 爬取所有文章中的详细内容



### 如何爬取所有页面上所有文章的链接？

- 直接去仓库下载所有文章
- 自己写一个爬虫爬取所有文章

为了练习一下爬虫，我自己写了一个爬虫代码。你可以下载我的爬虫进行使用。

#### 爬取单个页面上所有文章的链接

打开开发者工具，可以看到每个页面上分别有一个 <article> 节点包含的这个页面所显示的文章。

![image-20230520103439433](./README.assets/image-20230520103439433.png)

每个 <article> 节点下，有一个 rel="bookmark" 的 <a> 标签，里面包含文章的链接，我们只需要提取出这个链接即可。

#### 爬取多个页面上所有文章的链接

- #### 采用多线程的方案

  由于博客网站上有74页的内容，如果使用for循环逐一扒取每个页面上的链接，会消耗许多时间，所以这里才用多线程的方案进行多个页面的爬取。爬取页面过程是一个耗时的IO-bound任务（因为它需要访问网络）。如果用单线程依次执行这个任务，那么在等待网络响应的时候，CPU就会处于闲置状态，这是一种资源的浪费。而如果用`ThreadPoolExecutor`来并行执行这个任务，那么在某个线程等待网络响应的时候，其他线程可以继续执行任务，这样就可以提高效率。
  
- 爬取失败重试机制

  爬取页面的过程中，可能会因为网络或者其他的问题导致失败，这个时候要进行重新爬取该页面。

